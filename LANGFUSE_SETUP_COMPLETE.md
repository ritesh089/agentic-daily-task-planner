# ğŸ‰ LangFuse Integration Complete!

> **Complete LLM Observability** for your agentic framework with **zero code changes** in your agents!

---

## âœ… What Was Implemented

### 1. Framework Integration (Automatic LLM Tracing)

**Files Changed:**
- âœ… `framework/observability.py` - Added global LangFuse callback registration
- âœ… `config/observability_config.yaml` - LangFuse configuration
- âœ… `requirements.txt` - Added `langfuse>=2.0.0`
- âœ… `docs/OBSERVABILITY_GUIDE.md` - Complete documentation
- âœ… `LANGFUSE_INTEGRATION_SUMMARY.md` - Technical deep dive
- âœ… `README.md` - Updated features and setup

**How it works:**
```python
# framework/observability.py initializes once at startup
from langchain_core.callbacks.manager import configure

def init_langfuse():
    langfuse_handler = CallbackHandler(...)
    # Magic: Register GLOBALLY!
    configure(callbacks=[langfuse_handler])
    # Now ALL LLM calls are automatically traced!
```

**Your agent code (NO CHANGES!):**
```python
def my_agent(state):
    llm = ChatOllama(model="llama3")
    # This is automatically traced by LangFuse!
    response = llm.invoke("prompt")
    return {"response": response}
```

---

### 2. Docker Integration (Easy Setup)

**Files Changed:**
- âœ… `docker-compose.yml` - Added LangFuse service (v2)
- âœ… `env.example` - Environment variable template
- âœ… `README.md` - Updated installation guide

**What You Get:**
```
docker-compose up -d  â†’  3 Services Running
â”œâ”€ PostgreSQL (port 5432)  - Database for durability & LangFuse
â”œâ”€ Jaeger (port 16686)     - OTEL traces (agent orchestration)
â””â”€ LangFuse (port 3000)    - LLM traces (prompts, costs)
```

---

## ğŸš€ Quick Start

### Step 1: Start Services

```bash
cd /path/to/agentic-daily-task-planner

# Start all services
docker-compose up -d

# Wait ~30 seconds for LangFuse to initialize
docker-compose ps

# Should see all healthy:
# âœ… daily-task-planner-postgres
# âœ… daily-task-planner-jaeger
# âœ… daily-task-planner-langfuse
```

---

### Step 2: Setup LangFuse (First Time Only)

```bash
# 1. Open LangFuse UI
open http://localhost:3000

# 2. Create account (any email/password works locally)
#    Email: admin@localhost.com
#    Password: admin123

# 3. Get API keys
#    Click profile icon â†’ Settings â†’ API Keys â†’ Create new key
#    Copy:
#    - Public Key (pk-lf-...)
#    - Secret Key (sk-lf-...)

# 4. Create .env file
cp env.example .env

# 5. Edit .env and add your keys
nano .env

# Should contain:
LANGFUSE_NEXTAUTH_SECRET=changeme  # Generate: openssl rand -base64 32
LANGFUSE_SALT=changeme              # Generate: openssl rand -base64 32
LANGFUSE_PUBLIC_KEY=pk-lf-...       # From LangFuse UI
LANGFUSE_SECRET_KEY=sk-lf-...       # From LangFuse UI
LANGFUSE_HOST=http://localhost:3000

# 6. Export environment variables
export $(cat .env | xargs)

# 7. Verify
echo $LANGFUSE_PUBLIC_KEY  # Should print: pk-lf-...
```

---

### Step 3: Run Your Workflow

```bash
cd examples/conversational-assistant
python main.py

# You'll see:
# ğŸ“Š OTEL: Initialized
#    Exporters: otlp-traces
#    Captures: Agent spans, metrics, state transitions
#
# ğŸ“Š LangFuse: Initialized (GLOBAL auto-instrumentation)
#    Host: http://localhost:3000
#    ALL LLM calls automatically traced - zero code changes needed!
#    Linked to OTEL traces (correlated via trace ID)

# Now chat with the assistant...
You: What are my tasks today?
Assistant: Based on your emails...
```

---

### Step 4: View Traces

**LangFuse Dashboard:**
```bash
open http://localhost:3000/traces
```

You'll see:
- âœ… Every LLM call automatically captured
- âœ… Full prompt and response content
- âœ… Token usage (input/output/total)
- âœ… Cost estimates per call
- âœ… Latency metrics
- âœ… Session grouping (conversations)

**Jaeger Dashboard:**
```bash
open http://localhost:16686
```

You'll see:
- âœ… Complete agent orchestration traces
- âœ… State transitions between agents
- âœ… Duration of each agent
- âœ… Error tracking

**Correlation:**
- Both systems share the same trace ID
- Find slow LLM call in LangFuse â†’ Search trace ID in Jaeger â†’ See full agent context!

---

## ğŸ“Š What Gets Traced (Automatically!)

### Example Trace Flow

```
User Question: "What are my tasks today?"
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JAEGER (OTEL) - Application Traces                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ workflow_execution [2.5s] trace_id: abc123                 â”‚
â”‚ â”œâ”€ init_conversation [5ms]                                 â”‚
â”‚ â”œâ”€ get_user_input [150ms]                                  â”‚
â”‚ â”œâ”€ retrieve_context [80ms]                                 â”‚
â”‚ â”œâ”€ generate_response [2.2s] â† LLM call here!              â”‚
â”‚ â””â”€ display_response [20ms]                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LANGFUSE - LLM Traces                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ llm_generation [2.2s] trace_id: abc123                     â”‚
â”‚                                                             â”‚
â”‚ ğŸ“ Input (Prompt):                                          â”‚
â”‚    System: You are a helpful assistant...                  â”‚
â”‚    User: What are my tasks today?                          â”‚
â”‚    Context: [5 relevant emails]                            â”‚
â”‚                                                             â”‚
â”‚ ğŸ’¬ Output (Response):                                       â”‚
â”‚    Based on your emails, here are your tasks:              â”‚
â”‚    1. Review PR #123                                        â”‚
â”‚    2. Q4 Meeting at 2pm                                     â”‚
â”‚    3. ...                                                   â”‚
â”‚                                                             â”‚
â”‚ ğŸ“Š Metrics:                                                 â”‚
â”‚    Model: llama3.2                                          â”‚
â”‚    Input tokens: 150                                        â”‚
â”‚    Output tokens: 420                                       â”‚
â”‚    Total tokens: 570                                        â”‚
â”‚    Cost: $0.0057                                            â”‚
â”‚    Duration: 2,200ms                                        â”‚
â”‚    Temperature: 0.7                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Features

### âœ… Zero Code Changes
Your existing agents work without any modifications:
```python
# This code hasn't changed, but now it's traced!
def generate_response_agent(state):
    llm = ChatOllama(model="llama3.2")
    response = llm.invoke(messages)  # âœ… Automatically traced!
    return {"response": response.content}
```

### âœ… Complete Observability
**OTEL captures:**
- Agent execution spans
- State transitions
- Performance metrics
- Error tracking

**LangFuse captures:**
- Complete prompts
- Full responses
- Token usage
- Cost estimates
- Model metadata

### âœ… Easy Setup
```bash
# 3 commands to get everything running:
docker-compose up -d
export $(cat .env | xargs)
python main.py
```

### âœ… Production-Ready
- Persistent data storage
- Health checks
- Graceful restarts
- Secure credential management

---

## ğŸ› ï¸ Useful Commands

### Service Management
```bash
# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f langfuse-server
docker-compose logs -f jaeger

# Restart service
docker-compose restart langfuse-server

# Stop all
docker-compose down

# Fresh start (removes data!)
docker-compose down -v && docker-compose up -d
```

### Testing
```bash
# Test UI access
curl -I http://localhost:3000      # LangFuse (should be 200)
curl -I http://localhost:16686     # Jaeger (should be 200)

# Test database
docker exec -it daily-task-planner-postgres psql -U postgres -c "\l"
```

### Debugging
```bash
# Check LangFuse health
docker exec -it daily-task-planner-langfuse wget -O- http://localhost:3000/api/public/health

# View database connections
docker exec -it daily-task-planner-postgres \
  psql -U postgres -d langfuse -c "SELECT count(*) FROM traces;"
```

---

## ğŸ“š Documentation

- **[OBSERVABILITY_GUIDE.md](docs/OBSERVABILITY_GUIDE.md)** - Complete guide to dual observability
- **[LANGFUSE_INTEGRATION_SUMMARY.md](LANGFUSE_INTEGRATION_SUMMARY.md)** - Technical deep dive
- **[env.example](env.example)** - Environment variable template
- **[README.md](README.md)** - Updated installation guide

---

## ğŸ”’ Security Notes

### Local Development (Current Setup)
âœ… All data stays on your machine
âœ… No external services required
âœ… LangFuse runs in Docker locally

### Production Recommendations
1. **Generate Strong Secrets:**
   ```bash
   openssl rand -base64 32  # For NEXTAUTH_SECRET
   openssl rand -base64 32  # For SALT
   ```

2. **Use Environment Variables:**
   ```bash
   # Never commit these!
   export LANGFUSE_PUBLIC_KEY=pk-lf-...
   export LANGFUSE_SECRET_KEY=sk-lf-...
   ```

3. **Secure Database:**
   ```yaml
   # docker-compose.yml
   environment:
     POSTGRES_PASSWORD: ${DB_PASSWORD}  # From environment
   ```

4. **HTTPS for External Access:**
   - Use reverse proxy (nginx/traefik)
   - Add SSL certificates
   - Enable authentication

---

## ğŸ“ How It All Works Together

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Workflow                            â”‚
â”‚              (Zero observability code!)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  def my_agent(state):                                       â”‚
â”‚      response = llm.invoke("prompt")  # âœ… AUTO-TRACED!    â”‚
â”‚      return {"response": response}                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Framework Auto-Instrumentation                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                  â”‚
â”‚  ObservableStateGraph    â”‚  Global LangChain Callbacks     â”‚
â”‚  (OTEL)                  â”‚  (LangFuse)                      â”‚
â”‚  â€¢ Auto-instruments      â”‚  â€¢ Registered once at startup   â”‚
â”‚    agent nodes           â”‚  â€¢ Applies to ALL LLM calls     â”‚
â”‚  â€¢ Workflow traces       â”‚  â€¢ Captures prompts/responses   â”‚
â”‚  â€¢ State transitions     â”‚  â€¢ Tracks tokens/costs          â”‚
â”‚                          â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Docker Services                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                  â”‚
â”‚  Jaeger (16686)          â”‚  LangFuse (3000)                â”‚
â”‚  â€¢ Receives OTLP         â”‚  â€¢ Stores LLM traces            â”‚
â”‚  â€¢ Shows agent traces    â”‚  â€¢ Calculates costs             â”‚
â”‚  â€¢ Performance metrics   â”‚  â€¢ Groups sessions              â”‚
â”‚                          â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PostgreSQL Database (5432)                     â”‚
â”‚  â€¢ langgraph DB   (workflow checkpoints)                   â”‚
â”‚  â€¢ langfuse DB    (LLM trace data)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User makes request** â†’ Workflow starts
2. **ObservableStateGraph** â†’ Creates OTEL spans for each agent
3. **Agent calls LLM** â†’ Global LangFuse callback captures it
4. **LangFuse** â†’ Stores full prompt/response/cost in PostgreSQL
5. **OTEL** â†’ Sends agent traces to Jaeger
6. **Both systems** â†’ Use same trace ID for correlation

---

## ğŸ‰ Summary

### What You Achieved

âœ… **Complete Dual Observability**
- Application-level (OTEL): Agent orchestration, performance
- LLM-level (LangFuse): Prompts, responses, costs

âœ… **Zero Boilerplate**
- No code changes in agents
- Global auto-instrumentation
- Config-driven setup

âœ… **Production-Ready**
- All services in docker-compose
- Health checks and restarts
- Persistent data storage

âœ… **Easy to Use**
- 3 commands to start
- Web UIs for visualization
- Automatic trace correlation

### Next Steps

1. **âœ… Done:** All services running
2. **ğŸ¯ Next:** Open http://localhost:3000 and create account
3. **ğŸ¯ Next:** Get API keys and set environment variables
4. **ğŸ¯ Next:** Run your workflow and see traces!

---

## ğŸ’¡ Quick Reference

| What | URL | Purpose |
|------|-----|---------|
| **LangFuse** | http://localhost:3000 | LLM traces (prompts, costs) |
| **Jaeger** | http://localhost:16686 | Agent traces (orchestration) |
| **PostgreSQL** | localhost:5432 | Database (durability + traces) |

| Command | Purpose |
|---------|---------|
| `docker-compose up -d` | Start all services |
| `docker-compose ps` | Check status |
| `docker-compose logs -f` | View logs |
| `export $(cat .env \| xargs)` | Load environment variables |

---

**ğŸŠ Congratulations! Your framework now has world-class observability! ğŸŠ**

