# OpenTelemetry Observability Configuration
# Daily Task Planner Agent

# Enable/disable observability globally
enabled: true

# Service identification
service_name: "daily-task-planner-agent"
service_version: "1.0.0"

# Exporter configuration
exporters:
  # Console exporter (prints to stdout)
  console: false  # Disabled to reduce noise (you can set to true for debugging)
  
  # OTLP exporter (for Jaeger, Grafana, etc.)
  otlp:
    # Enable OTLP traces (works with Jaeger)
    traces: true
    # Disable OTLP metrics (Jaeger doesn't support metrics via OTLP)
    # Use an OpenTelemetry Collector or Prometheus for metrics
    metrics: false

# OTLP endpoint (if enabled)
# Default: local Jaeger or OTEL collector
otlp_endpoint: "http://localhost:4317"

# Trace sampling rate (0.0 to 1.0)
# 1.0 = sample all traces
# 0.1 = sample 10% of traces
sampling_rate: 1.0

# ============================================================================
# LangFuse Configuration (LLM-Specific Observability)
# ============================================================================
# LangFuse provides deep LLM observability:
# - Automatic prompt/response capture
# - Token usage and cost tracking
# - LLM chain visualization
# - Prompt versioning and evaluation
# - Human feedback collection
#
# How it works:
# - Automatically instruments ALL LLM calls via global callbacks
# - Zero code changes needed in your agents!
# - Works alongside OTEL (they complement each other)
#
# Setup:
# 1. Run LangFuse locally: docker run -p 3000:3000 langfuse/langfuse
# 2. Or use cloud: https://cloud.langfuse.com
# 3. Set environment variables:
#    export LANGFUSE_PUBLIC_KEY="pk-lf-..."
#    export LANGFUSE_SECRET_KEY="sk-lf-..."
#    export LANGFUSE_HOST="http://localhost:3000"  # or cloud URL
langfuse:
  # Enable LangFuse (default: true)
  enabled: true
  
  # LangFuse host (defaults to env var LANGFUSE_HOST)
  # host: "http://localhost:3000"
  
  # API keys (defaults to env vars LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY)
  # IMPORTANT: Use environment variables for security!
  # public_key: "pk-lf-..."
  # secret_key: "sk-lf-..."
  
  # Link LangFuse traces to OTEL traces (recommended)
  # This correlates LLM-level traces with application-level traces
  link_to_otel: true
  
  # What to capture (all enabled by default)
  capture:
    inputs: true        # LLM inputs (prompts)
    outputs: true       # LLM outputs (responses)
    metadata: true      # Model, temperature, etc.
    token_usage: true   # Token counts
    costs: true         # Cost estimates

# ============================================================================
# Observability Architecture
# ============================================================================
#
# This framework uses a DUAL observability approach:
#
# 1. OTEL (OpenTelemetry) - Application-Level Observability
#    - Captures: Agent execution spans, workflow traces, state transitions
#    - Automatic via: ObservableStateGraph
#    - Export to: Jaeger, Grafana, DataDog, etc.
#    - Use for: Understanding agent orchestration, performance bottlenecks
#
# 2. LangFuse - LLM-Specific Observability
#    - Captures: Prompts, responses, tokens, costs, reasoning chains
#    - Automatic via: Global LangChain callbacks
#    - Export to: LangFuse dashboard
#    - Use for: LLM debugging, cost optimization, prompt engineering
#
# Both are FULLY AUTOMATIC - your agents need ZERO code changes!
#
# ============================================================================
# Usage Instructions
# ============================================================================
# 
# Console Mode:
#   - Set exporters.console: true
#   - Traces and metrics print to stdout
#   - Good for development and debugging
#   - No external services required
#
# OTLP Mode with Jaeger (traces only):
#   1. Set exporters.otlp.traces: true
#   2. Set exporters.otlp.metrics: false (Jaeger doesn't support metrics)
#   3. Run Jaeger:
#      docker run -d -p 4317:4317 -p 16686:16686 jaegertracing/all-in-one
#   4. View traces at http://localhost:16686
#
# OTLP Mode with OpenTelemetry Collector (traces + metrics):
#   1. Set exporters.otlp.traces: true
#   2. Set exporters.otlp.metrics: true
#   3. Run OTEL Collector with Jaeger + Prometheus backends
#
# Disable Observability:
#   - Set enabled: false
#   - No performance impact
#   - Business logic unaffected
# ============================================================================

